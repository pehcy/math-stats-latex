\chapter{Limiting Distribution}

\section{Covergence in distribution}

Convergence in distribution state that 
the sequence of random variables $X_1, X_2, \ldots, X_n$ 
converges to some distribution $X$ when $n$ goes to infinity.
It does not require any dependence between the $X_n$ and $X$. Convergence in distribution is 
consider as the weakest type of convergence.

\begin{definition}[Covergence in distribution]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variable and let $X$ be a random variable. Let $F_{X_n}$ and 
    $F_X$ be the cdf of $X_n$ and $X$ respectively. And let $C(F)$ be the set of all continuous points of
    $F$. We say that $X_n$ converges in distribution to $X$ if 
    \begin{equation}
        \Lim{n \to \infty} F_{X_n}(x) = F_X(x)
    \end{equation}
    for all $x \in C(F)$. We denote $X_n \xrightarrow[]{d} X$. 
\end{definition}

\begin{example}
    Let $X_1, X_2, X_3, \ldots$ be a sequence of random variable such that 
    \[
        X_n \sim \text{Geom}(\lambda /n), \quad \forall n = 1,2,3,\ldots
    \]
    where $\lambda > 0$ is a constant. Define a new sequence $Y_n$ as 
    \[
        Y_n = \frac{1}{n}X_n, \quad \forall n = 1,2,3,\ldots
    \]
    Show that $Y_n$ converges in distribution to $\text{Exp}(\lambda)$.
\end{example}

\begin{solution}
    The cdf of $Y_n$ is 
    \begin{align*}
        F_{Y_n}(y) &= \mathbb{P}\left[ \frac{1}{n}X_n \leq y \right]\\
        &= \mathbb{P}\left[ X_n \leq ny \right]\\
        &= 1 - \left( 1- \frac{\lambda}{n} \right)^{\text{floor}(ny)}.
    \end{align*}
    and taking limit for $n \to +\infty$ we have 
    \begin{align*}
        \Lim{n \to \infty} \left( 1 - \left( 1- \frac{\lambda}{n} \right)^{\lfloor ny \rfloor } \right) 
        &= 1 - \Lim{n \to \infty} \left( 1 + \frac{1}{\left(-\mfrac{n}{\lambda} \right)} \right)^{-n(-y)}\\
        &= 1 - \Lim{n \to \infty} \left( 1 + \frac{1}{\left(-\mfrac{n}{\lambda}\right)} \right)^{-\mfrac{n}{\lambda}(-\lambda y)}\\
        &= 1 - e^{-\lambda y}
    \end{align*}
    which is the cdf of exponential distribution with parameter $\lambda$.
\end{solution}

\begin{example}
    Let $\{X_n \}_{n \geq 1}$ be a sequence of random variables with probability mass function 
    \[
        f_{X_n}(x) = \mathbb{P}[X_n = x] = \begin{cases}
            1 & \text{if } x = 2 + \mfrac{1}{n}\\
            0 & \text{otherwise}.
        \end{cases}
    \]
    Find the limiting distribution of $X_n$.
\end{example}

\subsection{Almost sure converges}

We said that $X_n$ converges to $X$ \textbf{almost surely} if the probability that the sequence $X_n(s)$ 
converges to $X(s)$ is equal to 1.

\begin{example}
    Consider the sample space $S = [0,1]$ with uniform probability distribution, for instance,
    \[
        \mathbb{P}([a,b]) = b - a \quad \forall 0 \leq a \leq b \leq 1.
    \]
    Define the sequence $\{X_n, n=1,2,3,\ldots\}$ as 
    \[
        X_n(s) = \frac{n}{n+1}s + (1-s)^n.
    \]
    Also, define the random variable $X$ on the sample space as $X(s) = s$. Show that $X_n$ 
    \textit{almost sure} converges to $X$.
\end{example}
\begin{solution}
    For any $s \in [0,1]$, taking the limit when $n >> \infty$ we have 
    \begin{align*}
        \Lim{n \to \infty}X_n(s) &= \Lim{n \to \infty} \left[ \frac{n}{n+1}s + (1 - s)^n \right]\\
        &= \Lim{n \to \infty} \frac{n}{n+1}s + \Lim{n \to \infty} (1 - s)^n\\
        &= 1\cdot s + 0\\
        &= s = X(s).
    \end{align*}
    However, if $s=0$ then 
    \[
        \Lim{n \to \infty}X_n(0) = \Lim{n \to \infty} \left[ \frac{n}{n+1}(0) + (1-0)^n\right] = 1.
    \]
    Thus, we conclude that $\Lim{n \to \infty} X_n(s) = X(s)$ for all $s$ lies between 0 and 1.
    And because $\mathbb{P}([0,1]) = 1$, we conclude that
    \[
        X_n \xrightarrow[]{a.s.} X
    \]
    and we are done.
\end{solution}

\section{Law of Large Numbers}

In this section we will discuss the Weak and Strong Law of Large Numbers. The Law of Large 
Numbers are consider as a form of convergence in probability.

We will first state the Weak Law of Large Numbers (WLLN),

\begin{theorem}[Weak Law of Large Numbers (WLLN)]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of i.i.d random variables, each with mean 
    $\mathbb{E}[X_i] = \mu$ and standard deviation $\sigma$, we define 
    \[
        \ols{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n}.
    \]
    The Weak Law of Large Numbers (WLLN) states that for all $\epsilon > 0$, we have 
    \begin{equation}
        \Lim{n \to \infty} \mathbb{P}[\, |\ols{X}_n - \mu| > \epsilon] = 0.
    \end{equation}
\end{theorem}

\begin{proof}
    Suppose that $Var[X_i] = \sigma^2 > 0$ for finite $i$. Since 
    $X_1, X_2, \ldots, X_n$ are identically independent, there is no 
    correlation between them, thus
    \begin{align*}
        Var[\ols{X}_n] &= Var \left[ \frac{X_1 + X_2 + \cdots + X_n}{n} \right]\\
        &= \frac{1}{n^2} Var[X_1 + X_2 + \cdots + X_n]\\
        &= \frac{1}{n^2} [Var X_1 + Var X_2 + \cdots + Var X_n]\\
        &= \frac{1}{n^2} (\underbrace{\sigma^2 + \sigma^2 + \cdots + \sigma^2}_{n \text{ times}})\\
        &= \frac{n\sigma^2}{n^2}
        = \frac{\sigma^2}{n}.
    \end{align*}

    Notice that the mean of each $X_i$ in the sequence is also equal to the mean of the sample average, 
    said $\mathbb{E}[X_i] = \mu$. We can now apply Chebyshev's inequality on $\ols{X}_n$ to get, for all 
    $\epsilon > 0$, 
    \[
        \mathbb{P}[\, |\ols{X}_n - \mu| > \epsilon] \leq \frac{\sigma^2}{n\epsilon^2}.
    \]
    So that 
\end{proof}

\begin{theorem}[Strong Law of Large Numbers]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of i.i.d random variables, each with mean 
    $\mathbb{E}[X_i] = \mu$ and standard deviation $\sigma$, then 
    \begin{equation}
        \mathbb{P}[\Lim{n \to \infty} \ols{X}_n = \mu] = 1. 
    \end{equation}
\end{theorem}

\begin{proof}
    By Markov's inequality that 
    \[
        \mathbb{P} \left[ \frac{1}{n}|\ols{X}_n - n\mu| \geq n^{-\gamma}\right] 
        \leq \frac{\mathbb{E}[(\mfrac{\ols{X}_n}{n} - \mu)^4]}{n^{-4\gamma}} = Kn^{-2+4\gamma}.
    \]
    Define for all $\gamma \in \left(0, \frac{1}{4}\right)$, and let 
    \[
        A_n = \left\{ \frac{1}{n}|\ols{X}_n - n\mu| \geq n^{-\gamma} \right\} \Rightarrow 
        \sum_{n \geq 1} \mathbb{P}[A_n] < \infty \Rightarrow 
        \mathbb{P}[A] = 0
    \]
    by the first Borel-Cantelli lemma, where $A = \bigcap_{n \geq 1} \bigcup_{m \geq n} A_n$. But now, the event 
    $A^c$ happened if and only if 
    \[
        \exists N\, \forall n \geq N \> \left| \frac{\ols{X}_n}{n} - \mu \right| < n^{-\gamma} 
        \Rightarrow \frac{\ols{X}_n}{n} \xrightarrow[]{p} \mu.
    \]
\end{proof}

\begin{theorem}[Central Limit Theorem]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of i.i.d random variable whose memoment generating function 
    exist in a neighborhood of 0. Let $\mathbb{E}[X_i] = \mu$ and $Var[X_i] = \sigma^2 > 0$. Define
    $\ols{X} = \mfrac{1}{n} \sum^n_{i=1} X_i$. Then 
    \begin{equation}
        Z = \frac{\sqrt{n} (\ols{X}_i - \mu)}{\sigma} \xrightarrow[]{d} \mathcal{N}(0,1)
    \end{equation}
    or 
    \begin{equation}
        Z = \frac{\sum^n_{i=1} X_i - n\mu}{\sqrt{n} \sigma} \xrightarrow[]{d} \mathcal{N}(0,1)
    \end{equation}
\end{theorem}

\begin{example}
    Let $\ols{X}_n$ be the sample mean from a random sampling of size 
    $n = 100$ from $\chi^2_{50}$. Compute approximate value of 
    $\mathbb{P}(49 < \ols{X} < 51)$.
\end{example}
\begin{solution}
    Because $\ols{X}$ followed Chi-squared distribution with degree of freedom 50, then the mean and variance are 
    $\mathbb{E}[X_i] = 50$ and $Var[X_i] = 2(50) = 100$. By Central Limit Theorem, 
    \begin{align*}
        \mathbb{P}(49 < \ols{X} < 51) &\simeq  
        \mathbb{P} \left[ \frac{\sqrt{100} (49 - \mathbb{E}[X_i])}{\sqrt{Var[X_i]}} < Z < \frac{\sqrt{100} (51 - \mathbb{E}[X_i])}{\sqrt{Var[X_i]}} \right]\\
        &\simeq \mathbb{P} \left[ \frac{\sqrt{100} (49 - 50)}{\sqrt{100}} < Z < \frac{\sqrt{100} (51 - 50)}{\sqrt{100}} \right]\\
        &\simeq \mathbb{P} \left[ -1 < Z < 1 \right]\\
        &\simeq \Phi(1) - \Phi(-1)\\
        &\simeq 0.84134 - 0.15866 = 0.68268.
    \end{align*} 
\end{solution}

\begin{theorem}[Slutsky's Theorem]
    If $X_n$ \textit{converges in distribution} to a random variable $X$, and $Y_n$ \textit{converges in probability} to a constant $c$, then 
    \begin{itemize}
        \item[$\bullet$] $Y_nX_n \xrightarrow[]{d} cX$
        \item[$\bullet$] $X_n + Y_n \xrightarrow[]{d} X + c$.
    \end{itemize}
\end{theorem}

\begin{proof}
    We will prove both parts of Slutsky's theorem.
    
    \textbf{Part 1:} We want to show that $Y_nX_n \xrightarrow{d} cX$.
    
    Since $Y_n \xrightarrow{p} c$, for any $\epsilon > 0$, we have $\mathbb{P}[|Y_n - c| > \epsilon] \to 0$ as $n \to \infty$.
    This implies that $Y_n$ is bounded in probability, and we can write $Y_n = c + (Y_n - c)$ where $(Y_n - c) \xrightarrow{p} 0$.
    
    Now, $Y_nX_n = cX_n + (Y_n - c)X_n$. Since $X_n \xrightarrow{d} X$ and multiplication by the constant $c$ is a continuous operation, we have $cX_n \xrightarrow{d} cX$.
    
    For the second term, we need to show that $(Y_n - c)X_n \xrightarrow{p} 0$. Since $Y_n - c \xrightarrow{p} 0$ and $X_n$ is bounded in probability (as it converges in distribution), their product converges to 0 in probability.
    
    By Slutsky's theorem for sums (which we prove next), we get $Y_nX_n = cX_n + (Y_n - c)X_n \xrightarrow{d} cX + 0 = cX$.
    
    \textbf{Part 2:} We want to show that $X_n + Y_n \xrightarrow{d} X + c$.
    
    We use characteristic functions. Let $\phi_{X_n}(t)$, $\phi_X(t)$, and $\phi_{Y_n}(t)$ denote the characteristic functions of $X_n$, $X$, and $Y_n$, respectively.
    
    The characteristic function of $X_n + Y_n$ is given by:
    \begin{align*}
        \phi_{X_n + Y_n}(t) &= \mathbb{E}[e^{it(X_n + Y_n)}] = \mathbb{E}[e^{itX_n}e^{itY_n}]
    \end{align*}
    
    Since $Y_n \xrightarrow{p} c$, we have $e^{itY_n} \xrightarrow{p} e^{itc}$ by the continuous mapping theorem.
    
    Using the fact that $X_n \xrightarrow{d} X$ implies $\phi_{X_n}(t) \to \phi_X(t)$, and that convergence in probability preserves the limit of expectations for bounded random variables, we get:
    \begin{align*}
        \lim_{n \to \infty} \phi_{X_n + Y_n}(t) &= \lim_{n \to \infty} \mathbb{E}[e^{itX_n}e^{itY_n}] \\
        &= \mathbb{E}[e^{itX}] \cdot e^{itc} \\
        &= \phi_X(t) \cdot e^{itc} \\
        &= \phi_{X + c}(t)
    \end{align*}
    
    Since the characteristic function of $X_n + Y_n$ converges pointwise to the characteristic function of $X + c$, we conclude that $X_n + Y_n \xrightarrow{d} X + c$.
\end{proof}

\begin{example}
    If the random variable $X \sim \text{Gamma}(\mu, 1)$, show that 
    \[
        \frac{\sqrt{n}(\ols{X}_n - \mu)}{\sqrt{\ols{X}_n}} \xrightarrow[]{d} \mathcal{N}(0,1)
    \]
\end{example}
\begin{solution}
    Slutsky's theorem stated that If $X_n$ converges in distribution to a random variable 
    $X$ and if $Y_n$ converges in probability to a constant $c$. Then $X_n / Y_n$
    converges in distribution to $X/c$. By the central limit theorem we have 
    \[
        \sqrt{n}(\ols{X}_n - \mu) \xrightarrow[]{d} \mathcal{N}(0, Var[X_i]).
    \]
    and in this case $\mathbb{E}X_i = Var[X_i] = \mu$, thus we obtained
    \[
        \sqrt{n}(\ols{X}_n - \mu) \xrightarrow[]{d} \mathcal{N}(0, \mu).
    \]
    Replacing the theorem denominator $Y_n$ with $\ols{X}_n$, which $\ols{X}_n$ converges to constant $\mu$ 
    in probability. Hence
    \[
        \frac{\sqrt{n}(\ols{X}_n - \mu)}{\sqrt{\ols{X}_n}} \xrightarrow[]{d} \mathcal{N}\left(\frac{0}{\mu}, \frac{\mu}{\mu}\right)
        \Longrightarrow \frac{\sqrt{n}(\ols{X}_n - \mu)}{\sqrt{\ols{X}_n}} \xrightarrow[]{d} \mathcal{N}(0,1)
    \]
    and we are done with the proof.
\end{solution}

\begin{theorem}
    If the random variable $X_n$ converges to constant $c$ in probability, then 
    \begin{equation}
        \sqrt{X_n} \xrightarrow[]{p} \sqrt{c},\quad c > 0. 
    \end{equation}
\end{theorem}

\begin{theorem}
    If the random variable $X_n$ converges to constant $c$ in probability, and 
    $Y_n$ converges to constant $d$ in probability, then 
    \begin{itemize}
        \item[$\bullet$] $aX_n + bY_n \xrightarrow[]{p} ac + bd$.
        \item[$\bullet$] $X_n Y_n \xrightarrow[]{p} cd$.
        \item[$\bullet$] $\mfrac{1}{X_n} \xrightarrow[]{p} \mfrac{1}{c}$ for all $c \neq 0$.
    \end{itemize}
\end{theorem}

\begin{proof}
    We will prove each part of the theorem.
    
    \textbf{Part 1:} We want to show that $aX_n + bY_n \xrightarrow{p} ac + bd$.
    
    For any $\epsilon > 0$, we have:
    \begin{align*}
        |aX_n + bY_n - (ac + bd)| &= |a(X_n - c) + b(Y_n - d)| \\
        &\leq |a||X_n - c| + |b||Y_n - d|
    \end{align*}
    
    By the triangle inequality, for any $\delta > 0$:
    \begin{align*}
        &\mathbb{P}[|aX_n + bY_n - (ac + bd)| > \epsilon] \\
        &\leq \mathbb{P}[|a||X_n - c| + |b||Y_n - d| > \epsilon] \\
        &\leq \mathbb{P}[|a||X_n - c| > \epsilon/2] + \mathbb{P}[|b||Y_n - d| > \epsilon/2] \\
        &= \mathbb{P}[|X_n - c| > \mfrac{\epsilon}{2a}] + \mathbb{P}[|Y_n - d| > \mfrac{\epsilon}{2b}], \quad \forall a, b > 0
    \end{align*}
    
    Since $X_n \xrightarrow{p} c$ and $Y_n \xrightarrow{p} d$, both terms on the right approach 0 as $n \to \infty$.
    
    \textbf{Part 2:} We want to show that $X_n Y_n \xrightarrow{p} cd$.
    
    We can write:
    \begin{align*}
        X_n Y_n - cd &= X_n Y_n - cY_n + cY_n - cd \\
        &= Y_n(X_n - c) + c(Y_n - d)
    \end{align*}
    
    Since $Y_n \xrightarrow{p} d$, the sequence $\{Y_n\}$ is bounded in probability. That is, for any $\delta > 0$, there exists $M > 0$ such that $\mathbb{P}[|Y_n| > M] < \delta$ for all $n$ sufficiently large.
    
    For any $\epsilon > 0$:
    \begin{align*}
        |X_n Y_n - cd| &= |Y_n(X_n - c) + c(Y_n - d)| \\
        &\leq |Y_n||X_n - c| + |c||Y_n - d|
    \end{align*}
    
    Given $\epsilon > 0$, choose $\delta > 0$ such that:
    \begin{align*}
        &\mathbb{P}[|X_n Y_n - cd| > \epsilon] \\
        &\leq \mathbb{P}[|Y_n||X_n - c| + |c||Y_n - d| > \epsilon] \\
        &\leq \mathbb{P}[|Y_n||X_n - c| > \epsilon/2] + \mathbb{P}[|c||Y_n - d| > \epsilon/2]
    \end{align*}
    
    For the first term, using the boundedness of $Y_n$ and convergence of $X_n$, and for the second term using convergence of $Y_n$, both approach 0 as $n \to \infty$.
    
    \textbf{Part 3:} We want to show that $\frac{1}{X_n} \xrightarrow{p} \frac{1}{c}$ for $c \neq 0$.
    
    Since $c \neq 0$, there exists $\delta > 0$ such that $|c| > \delta > 0$. Because $X_n \xrightarrow{p} c$, for any $\epsilon > 0$, we have $\mathbb{P}[|X_n - c| > \epsilon] \to 0$.
    
    In particular, $\mathbb{P}[|X_n - c| > \delta/2] \to 0$, which implies $\mathbb{P}[|X_n| > \delta/2] \to 1$. This means $X_n$ is bounded away from 0 in probability.
    
    Now, for any $\epsilon > 0$:
    \begin{align*}
        \left|\frac{1}{X_n} - \frac{1}{c}\right| &= \left|\frac{c - X_n}{X_n c}\right| = \frac{|X_n - c|}{|X_n||c|}
    \end{align*}
    
    On the event $\{|X_n| > \delta/2\}$, we have:
    \begin{align*}
        \left|\frac{1}{X_n} - \frac{1}{c}\right| \leq \frac{2|X_n - c|}{\delta|c|}
    \end{align*}
    
    Therefore:
    \begin{align*}
        \mathbb{P}\left[\left|\frac{1}{X_n} - \frac{1}{c}\right| > \epsilon\right] &\leq \mathbb{P}[|X_n| \leq \delta/2] + \mathbb{P}\left[\frac{2|X_n - c|}{\delta|c|} > \epsilon, |X_n| > \delta/2\right] \\
        &\leq \mathbb{P}[|X_n| \leq \delta/2] + \mathbb{P}\left[|X_n - c| > \frac{\epsilon\delta|c|}{2}\right]
    \end{align*}
    
    As $n \to \infty$, both terms vanishing to zero, this completes the proof.
\end{proof}

\section*{Tutorials}

\begin{mdframed}
    \vspace{-0.25cm}
    \hspace{-0.25cm}
    \begin{Exercise}
        Let $X_1, X_2, \ldots, X_{2020}$ be a random sample of size $2020$ 
        from a Poisson distribution with density function
        \[
            f_{X_i}(x) = \frac{e^{-\lambda} \lambda^x}{x!}, \quad x = 0,1,2,\ldots,\infty.
        \]
        What is the distribution of $2020 \ols{X}$?
    \end{Exercise}
\end{mdframed}