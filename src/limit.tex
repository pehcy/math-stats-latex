\chapter{Limiting Distribution}

\section{Probability inequality}
We first start off with some useful probability inequalities that will be used later in this chapter -- the inequalities 
which contain the probability measure in either left side or right side or in both sides. This is known as
\textbf{probability inequalities}.

\begin{theorem}[Markov's Inequality]
    Suppose $X$ is a random variable that having finite expectation, said $\mathbb{E}[X]$ converges. Then 
    for any nonzero quantity $a$ are having the inequality
    \begin{equation}
        \mathbb{P}[X \geq a] \leq \frac{\mathbb{E}[X]}{a}.
    \end{equation}
\end{theorem}

\section{Covergence in distribution}

Convergence in distribution state that 
the sequence of random variables $X_1, X_2, \ldots, X_n$ 
converges to some distribution $X$ when $n$ goes to infinity.
It does not require any dependence between the $X_n$ and $X$. Convergence in distribution is 
consider as the weakest type of convergence. The sequence $\{X_n\}$ is 
said to weakly converges to $X$ if the cdf $F_n(x) \to F(x)$ whenever $n \to \infty$ at all 
continuity points of $F(x)$. Symbolically, we denote this as $X_n \xrightarrow[]{\mathcal{D}} X$.

\begin{definition}[Covergence in distribution]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variable and let $X$ be a random variable. Let $F_{X_n}$ and 
    $F_X$ be the cdf of $X_n$ and $X$ respectively. And let $C(F)$ be the set of all continuous points of
    $F$. We say that $X_n$ converges in distribution to $X$ if 
    \begin{equation}
        \Lim{n \to \infty} F_{X_n}(x) = F_X(x)
    \end{equation}
    for all $x \in C(F)$. We denote $X_n \xrightarrow[]{\mathcal{D}} X$. 
\end{definition}

\begin{example}
    Let $X_1, X_2, X_3, \ldots$ be a sequence of random variable such that 
    \[
        X_n \sim \text{Geom}(\lambda /n), \quad \forall n = 1,2,3,\ldots
    \]
    where $\lambda > 0$ is a constant. Define a new sequence $Y_n$ as 
    \[
        Y_n = \frac{1}{n}X_n, \quad \forall n = 1,2,3,\ldots
    \]
    Show that $Y_n$ converges in distribution to $\text{Exp}(\lambda)$.
\end{example}

\begin{solution}
    The cdf of $Y_n$ is 
    \begin{align*}
        F_{Y_n}(y) &= \mathbb{P}\left[ \frac{1}{n}X_n \leq y \right]\\
        &= \mathbb{P}\left[ X_n \leq ny \right]\\
        &= 1 - \left( 1- \frac{\lambda}{n} \right)^{\text{floor}(ny)}.
    \end{align*}
    and taking limit for $n \to +\infty$ we have 
    \begin{align*}
        \Lim{n \to \infty} \left( 1 - \left( 1- \frac{\lambda}{n} \right)^{\lfloor ny \rfloor } \right) 
        &= 1 - \Lim{n \to \infty} \left( 1 + \frac{1}{\left(-\mfrac{n}{\lambda} \right)} \right)^{-n(-y)}\\
        &= 1 - \Lim{n \to \infty} \left( 1 + \frac{1}{\left(-\mfrac{n}{\lambda}\right)} \right)^{-\mfrac{n}{\lambda}(-\lambda y)}\\
        &= 1 - e^{-\lambda y}
    \end{align*}
    which is the cdf of exponential distribution with parameter $\lambda$.
\end{solution}

\begin{example}
    Let $\{X_n \}_{n \geq 1}$ be a sequence of random variables drawn from a uniform distribution $(0, \theta)$ distribution.
    We define the order statistics
    \[
        X_{(n)} = \max \{ X_1, X_2, \ldots, X_n \}.
    \]
    Show that $\Lim{n \to \infty} X_{(n)} = \theta$.
\end{example}
\begin{solution}
    From the cdf of $X_{(n)}$ we have 
    \begin{align*}
        \mathbb{P}[X_{(n)} \leq x] &= F_n(x)\\
        &= \mathbb{P}[X_1 , X_2, \ldots, X_n \leq x]\\
        &= \prod^n_{i=1} \mathbb{P}[X_i \leq x] & \text{Since all } X_i \text{ are identically independent }\\
        &= \left( \mathbb{P}[X_1 \leq x]\right)^n\\
        &= \begin{cases}
            0 & \text{if } x < 0\\
            \left(\displaystyle \frac{x}{\theta}\right)^n & \text{if } 0 \leq x < \theta\\
            1 & \text{if } x > \theta\\
        \end{cases}
    \end{align*}
    which is the joint density function of $n$ uniform distribution. Note that
    \[
        F_n(x) \xrightarrow[]{p} F(x) = \begin{cases}
            0 & \text{if } x < \theta\\
            1 & \text{if } x \geq \theta
        \end{cases}
    \]
    and now we can see that the limiting distribution become degenerate at $\theta$.
\end{solution}

\begin{definition}[Converges in probability]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variable defined on some probability space 
    $(\Omega, \mathscr{F}, P)$, and let $X$ be a random variable. We say that the sequence
    $X_n$ \textbf{converges in probability} to $X$ if for all $\epsilon > 0$, we have 
    \begin{equation}
        \mathbb{P} \left\{ |X_n - X| > \epsilon \right\} \to 0 \quad \text{as } n \to \infty.
    \end{equation}
    Or, equivalently,
    \begin{equation}
        \mathbb{P} \left\{ |X_n - X| < \epsilon \right\} \to 1 \quad \text{as } n \to \infty.
    \end{equation}
    We denote this as $X_n \xrightarrow[]{p} X$.
\end{definition}
\begin{remark}
    We emphasize that the definition of convergence in probability says nothing about the convergence of the random variables
    $X_n$ in the sense in which it is understood in real analysis.

    In particular, $X_n \xrightarrow[]{p} X$ does not imply that given $\epsilon > 0$, we can find an integer 
    $N$ such that $|X_n - X| < \epsilon$ for all $n \geq N$. The definition above speaks about the 
    convergence of the sequence of probabilities $\mathbb{P}\{|X_n - X| > \epsilon\}$ to 0 as $n$ goes to infinity.
\end{remark}

\begin{example}
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variable with probability mass function
    \[
        \mathbb{P}[X_n = 1] = \frac{1}{n}, \quad \mathbb{P}[X_n = 0] = 1 - \frac{1}{n}.
    \]
    Show that $X_n$ converges in probability to 0.
\end{example}
\begin{solution}
    From the definition of convergence in probability, we need to show that for all $\epsilon > 0$, that is,
    \[
        \mathbb{P} \left( |X_n| > \epsilon \right) = \begin{cases}
            \mathbb{P}(X_n = 1) = \frac{1}{n} & \text{if } 0 < \epsilon < 1\\
            0 & \text{if } \epsilon \geq 1.
        \end{cases}
    \]
    It follows that $\mathbb{P} \left( |X_n| > \epsilon \right) \to 0$ as $n \to \infty$, which proves that $X_n \xrightarrow[]{p} 0$.
\end{solution}


\begin{theorem}[Slutsky's Theorem]
    If $X_n$ \textit{converges in distribution} to a random variable $X$, and $Y_n$ \textit{converges in probability} to a constant $c$, then 
    \begin{itemize}
        \item[$\bullet$] $Y_nX_n \xrightarrow[]{\mathcal{D}} cX$
        \item[$\bullet$] $X_n + Y_n \xrightarrow[]{\mathcal{D}} X + c$.
    \end{itemize}
\end{theorem}

\begin{proof}
    We will prove both parts of Slutsky's theorem.
    
    \textbf{Part 1:} We want to show that $Y_nX_n \xrightarrow{d} cX$.
    
    Since $Y_n \xrightarrow{p} c$, for any $\epsilon > 0$, we have $\mathbb{P}[|Y_n - c| > \epsilon] \to 0$ as $n \to \infty$.
    This implies that $Y_n$ is bounded in probability, and we can write $Y_n = c + (Y_n - c)$ where $(Y_n - c) \xrightarrow{p} 0$.
    
    Now, $Y_nX_n = cX_n + (Y_n - c)X_n$. Since $X_n \xrightarrow{d} X$ and multiplication by the constant $c$ is a continuous operation, we have $cX_n \xrightarrow{d} cX$.
    
    For the second term, we need to show that $(Y_n - c)X_n \xrightarrow{p} 0$. Since $Y_n - c \xrightarrow{p} 0$ and $X_n$ is bounded in probability (as it converges in distribution), their product converges to 0 in probability.
    
    By Slutsky's theorem for sums (which we prove next), we get $Y_nX_n = cX_n + (Y_n - c)X_n \xrightarrow{d} cX + 0 = cX$.
    
    \textbf{Part 2:} We want to show that $X_n + Y_n \xrightarrow{d} X + c$.
    
    We use characteristic functions. Let $\phi_{X_n}(t)$, $\phi_X(t)$, and $\phi_{Y_n}(t)$ denote the characteristic functions of $X_n$, $X$, and $Y_n$, respectively.
    
    The characteristic function of $X_n + Y_n$ is given by:
    \begin{align*}
        \phi_{X_n + Y_n}(t) &= \mathbb{E}[e^{it(X_n + Y_n)}] = \mathbb{E}[e^{itX_n}e^{itY_n}]
    \end{align*}
    
    Since $Y_n \xrightarrow{p} c$, we have $e^{itY_n} \xrightarrow{p} e^{itc}$ by the continuous mapping theorem.
    
    Using the fact that $X_n \xrightarrow{d} X$ implies $\phi_{X_n}(t) \to \phi_X(t)$, and that convergence in probability preserves the limit of expectations for bounded random variables, we get:
    \begin{align*}
        \lim_{n \to \infty} \phi_{X_n + Y_n}(t) &= \lim_{n \to \infty} \mathbb{E}[e^{itX_n}e^{itY_n}] \\
        &= \mathbb{E}[e^{itX}] \cdot e^{itc} \\
        &= \phi_X(t) \cdot e^{itc} \\
        &= \phi_{X + c}(t)
    \end{align*}
    
    Since the characteristic function of $X_n + Y_n$ converges pointwise to the characteristic function of $X + c$, we conclude that $X_n + Y_n \xrightarrow{d} X + c$.
\end{proof}

\begin{example}
    If the random variable $X \sim \text{Gamma}(\mu, 1)$, show that 
    \[
        \frac{\sqrt{n}(\ols{X}_n - \mu)}{\sqrt{\ols{X}_n}} \xrightarrow[]{\mathcal{D}} \mathcal{N}(0,1)
    \]
\end{example}
\begin{solution}
    Slutsky's theorem stated that If $X_n$ converges in distribution to a random variable 
    $X$ and if $Y_n$ converges in probability to a constant $c$. Then $X_n / Y_n$
    converges in distribution to $X/c$. By the central limit theorem we have 
    \[
        \sqrt{n}(\ols{X}_n - \mu) \xrightarrow[]{\mathcal{D}} \mathcal{N}(0, Var[X_i]).
    \]
    and in this case $\mathbb{E}X_i = Var[X_i] = \mu$, thus we obtained
    \[
        \sqrt{n}(\ols{X}_n - \mu) \xrightarrow[]{\mathcal{D}} \mathcal{N}(0, \mu).
    \]
    Replacing the theorem denominator $Y_n$ with $\ols{X}_n$, which $\ols{X}_n$ converges to constant $\mu$ 
    in probability. Hence
    \[
        \frac{\sqrt{n}(\ols{X}_n - \mu)}{\sqrt{\ols{X}_n}} \xrightarrow[]{\mathcal{D}} \mathcal{N}\left(\frac{0}{\mu}, \frac{\mu}{\mu}\right)
        \Longrightarrow \frac{\sqrt{n}(\ols{X}_n - \mu)}{\sqrt{\ols{X}_n}} \xrightarrow[]{\mathcal{D}} \mathcal{N}(0,1)
    \]
    and we are done with the proof.
\end{solution}

\begin{theorem}
    If the random variable $X_n$ converges to constant $c$ in probability, then 
    \begin{equation}
        \sqrt{X_n} \xrightarrow[]{p} \sqrt{c},\quad c > 0. 
    \end{equation}
\end{theorem}

\begin{theorem}
    If the random variable $X_n$ converges to constant $c$ in probability, and 
    $Y_n$ converges to constant $d$ in probability, then 
    \begin{itemize}
        \item[$\bullet$] $aX_n + bY_n \xrightarrow[]{p} ac + bd$.
        \item[$\bullet$] $X_n Y_n \xrightarrow[]{p} cd$.
        \item[$\bullet$] $\mfrac{1}{X_n} \xrightarrow[]{p} \mfrac{1}{c}$ for all $c \neq 0$.
    \end{itemize}
\end{theorem}

\begin{proof}
    We will prove each part of the theorem.
    
    \textbf{Part 1:} We want to show that $aX_n + bY_n \xrightarrow{p} ac + bd$.
    
    For any $\epsilon > 0$, we have:
    \begin{align*}
        |aX_n + bY_n - (ac + bd)| &= |a(X_n - c) + b(Y_n - d)| \\
        &\leq |a||X_n - c| + |b||Y_n - d|
    \end{align*}
    
    By the triangle inequality, for any $\delta > 0$:
    \begin{align*}
        &\mathbb{P}[|aX_n + bY_n - (ac + bd)| > \epsilon] \\
        &\leq \mathbb{P}[|a||X_n - c| + |b||Y_n - d| > \epsilon] \\
        &\leq \mathbb{P}[|a||X_n - c| > \epsilon/2] + \mathbb{P}[|b||Y_n - d| > \epsilon/2] \\
        &= \mathbb{P}[|X_n - c| > \mfrac{\epsilon}{2a}] + \mathbb{P}[|Y_n - d| > \mfrac{\epsilon}{2b}], \quad \forall a, b > 0
    \end{align*}
    
    Since $X_n \xrightarrow{p} c$ and $Y_n \xrightarrow{p} d$, both terms on the right approach 0 as $n \to \infty$.
    
    \textbf{Part 2:} We want to show that $X_n Y_n \xrightarrow{p} cd$.
    
    We can write:
    \begin{align*}
        X_n Y_n - cd &= X_n Y_n - cY_n + cY_n - cd \\
        &= Y_n(X_n - c) + c(Y_n - d)
    \end{align*}
    
    Since $Y_n \xrightarrow{p} d$, the sequence $\{Y_n\}$ is bounded in probability. That is, for any $\delta > 0$, there exists $M > 0$ such that $\mathbb{P}[|Y_n| > M] < \delta$ for all $n$ sufficiently large.
    
    For any $\epsilon > 0$:
    \begin{align*}
        |X_n Y_n - cd| &= |Y_n(X_n - c) + c(Y_n - d)| \\
        &\leq |Y_n||X_n - c| + |c||Y_n - d|
    \end{align*}
    
    Given $\epsilon > 0$, choose $\delta > 0$ such that:
    \begin{align*}
        &\mathbb{P}[|X_n Y_n - cd| > \epsilon] \\
        &\leq \mathbb{P}[|Y_n||X_n - c| + |c||Y_n - d| > \epsilon] \\
        &\leq \mathbb{P}[|Y_n||X_n - c| > \epsilon/2] + \mathbb{P}[|c||Y_n - d| > \epsilon/2]
    \end{align*}
    
    For the first term, using the boundedness of $Y_n$ and convergence of $X_n$, and for the second term using convergence of $Y_n$, both approach 0 as $n \to \infty$.
    
    \textbf{Part 3:} We want to show that $\frac{1}{X_n} \xrightarrow{p} \frac{1}{c}$ for $c \neq 0$.
    
    Since $c \neq 0$, there exists $\delta > 0$ such that $|c| > \delta > 0$. Because $X_n \xrightarrow{p} c$, for any $\epsilon > 0$, we have $\mathbb{P}[|X_n - c| > \epsilon] \to 0$.
    
    In particular, $\mathbb{P}[|X_n - c| > \delta/2] \to 0$, which implies $\mathbb{P}[|X_n| > \delta/2] \to 1$. This means $X_n$ is bounded away from 0 in probability.
    
    Now, for any $\epsilon > 0$:
    \begin{align*}
        \left|\frac{1}{X_n} - \frac{1}{c}\right| &= \left|\frac{c - X_n}{X_n c}\right| = \frac{|X_n - c|}{|X_n||c|}
    \end{align*}
    
    On the event $\{|X_n| > \delta/2\}$, we have:
    \begin{align*}
        \left|\frac{1}{X_n} - \frac{1}{c}\right| \leq \frac{2|X_n - c|}{\delta|c|}
    \end{align*}
    
    Therefore:
    \begin{align*}
        \mathbb{P}\left[\left|\frac{1}{X_n} - \frac{1}{c}\right| > \epsilon\right] &\leq \mathbb{P}[|X_n| \leq \delta/2] + \mathbb{P}\left[\frac{2|X_n - c|}{\delta|c|} > \epsilon, |X_n| > \delta/2\right] \\
        &\leq \mathbb{P}[|X_n| \leq \delta/2] + \mathbb{P}\left[|X_n - c| > \frac{\epsilon\delta|c|}{2}\right]
    \end{align*}
    
    As $n \to \infty$, both terms vanishing to zero, this completes the proof.
\end{proof}

\begin{definition}[Almost sure converges]
We said that $X_n$ converges to $X$ \textbf{almost surely} if the probability that the sequence $X_n(s)$ 
converges to $X(s)$ is equal to 1.
\end{definition}

\begin{example}
    Consider the sample space $S = [0,1]$ with uniform probability distribution, for instance,
    \[
        \mathbb{P}([a,b]) = b - a \quad \forall 0 \leq a \leq b \leq 1.
    \]
    Define the sequence $\{X_n, n=1,2,3,\ldots\}$ as 
    \[
        X_n(s) = \frac{n}{n+1}s + (1-s)^n.
    \]
    Also, define the random variable $X$ on the sample space as $X(s) = s$. Show that $X_n$ 
    \textit{almost sure} converges to $X$.
\end{example}
\begin{solution}
    For any $s \in [0,1]$, taking the limit when $n >> \infty$ we have 
    \begin{align*}
        \Lim{n \to \infty}X_n(s) &= \Lim{n \to \infty} \left[ \frac{n}{n+1}s + (1 - s)^n \right]\\
        &= \Lim{n \to \infty} \frac{n}{n+1}s + \Lim{n \to \infty} (1 - s)^n\\
        &= 1\cdot s + 0\\
        &= s = X(s).
    \end{align*}
    However, if $s=0$ then 
    \[
        \Lim{n \to \infty}X_n(0) = \Lim{n \to \infty} \left[ \frac{n}{n+1}(0) + (1-0)^n\right] = 1.
    \]
    Thus, we conclude that $\Lim{n \to \infty} X_n(s) = X(s)$ for all $s$ lies between 0 and 1.
    And because $\mathbb{P}([0,1]) = 1$, we conclude that
    \[
        X_n \xrightarrow[]{a.s.} X
    \]
    and we are done.
\end{solution}

\section{Law of Large Numbers}

In this section we will discuss the Weak and Strong Law of Large Numbers. The Law of Large 
Numbers are consider as a form of convergence in probability.

In practice estimates are made of an unknown quantity (or parameter) by taking the average $\ols{X}_n$
of a number of repeated measurements of the quantity each of which may have been affected by random errors. It is, 
therefore, of interest to study the properties of such an estimate. An initial enquiry is made concerning its behavior 
as the number of measurements increases without bound (as $n \to \infty$). The Law of Large Numbers gives an answer to this question.
Does the estimate $\ols{X}_n$ converge to the true value $\xi$ of the parameter under study?

The probability can be formulated as follows: Let $X_1, X_2, \ldots, X_n$ be a sequence of i.i.d observations and $\ols{X}_n$ 
be the mean of the first $n$ observations. Under what condition can we say that 
\[
    \ols{X}_n \to \xi \quad \text{as } n \to \infty?
\]
In one or other sense we shall generalise this problem further and ask for the conditions under which
\[
    \ols{X}_n - \overline{\xi}_n \to 0 \quad \text{as } n \to \infty,
\]
where $\{\xi_n \}$ is a sequence of constants that is measured by the sequence of observations $\{X_n \}$.
We shall say that the Weak Law of Large Numbers (WLLN) holds if the convergence such that 
\begin{equation}
    \ols{X}_n \to \xi \quad \text{ or } \quad (\ols{X}_n - \overline{\xi}_n) \to 0 
\end{equation}
takes place.

When the convergence is in probability we shall said that WLLN holds. Thus the theorem on WLLN states the 
conditions under which the WLLN holds for a given sequence of observations $\{X_n \}$.

In other words, our problem is to answer the question in the affirmative sense that whether there exists 
a sequence of constants $\{A_n \}$ and $\{B_n \}$ such that $B_n \to \infty$ as $n \to \infty$ and such that 
\begin{equation}
    \frac{\sum^n_{k=1} X_k - A_n}{B_n} \xrightarrow[]{p} 0 \quad \text{as } n \to \infty.
\end{equation}

\begin{theorem}[Weak Law of Large Numbers (WLLN)]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of i.i.d random variables, each with mean 
    $\mathbb{E}[X_i] = \mu$ and standard deviation $\sigma$, we define 
    \[
        \ols{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n}.
    \]
    The Weak Law of Large Numbers (WLLN) states that for all $\epsilon > 0$, we have 
    \begin{equation}
        \Lim{n \to \infty} \mathbb{P}[\, |\ols{X}_n - \mu| > \epsilon] = 0.
    \end{equation}
\end{theorem}

\begin{proof}
    Suppose that $Var[X_i] = \sigma^2 > 0$ for finite $i$. Since 
    $X_1, X_2, \ldots, X_n$ are identically independent, there is no 
    correlation between them, thus
    \begin{align*}
        Var[\ols{X}_n] &= Var \left[ \frac{X_1 + X_2 + \cdots + X_n}{n} \right]\\
        &= \frac{1}{n^2} Var[X_1 + X_2 + \cdots + X_n]\\
        &= \frac{1}{n^2} [Var X_1 + Var X_2 + \cdots + Var X_n]\\
        &= \frac{1}{n^2} (\underbrace{\sigma^2 + \sigma^2 + \cdots + \sigma^2}_{n \text{ times}})\\
        &= \frac{n\sigma^2}{n^2}
        = \frac{\sigma^2}{n}.
    \end{align*}

    Notice that the mean of each $X_i$ in the sequence is also equal to the mean of the sample average, 
    said $\mathbb{E}[X_i] = \mu$. We can now apply Chebyshev's inequality on $\ols{X}_n$ to get, for all 
    $\epsilon > 0$, 
    \[
        \mathbb{P}[\, |\ols{X}_n - \mu| > \epsilon] \leq \frac{\sigma^2}{n\epsilon^2}.
    \]
    So that 
\end{proof}

\begin{lemma}
    Denote $Z$ the ``standard normal random variable`` with density $\frac{1}{\sqrt{2\pi}} e^{-x^2/2}$. Then
    \begin{equation}
        \uexp[x]{e^{itZ}} = e^{-t^2/2}.
    \end{equation}
\end{lemma}
\begin{proof}
    We may use the same calculation as for the moment generating function:
    \[
        \int^\infty_{-\infty} \exp \left( itx - \frac{1}{2}x^2 \right) \> \mathrm{d}x 
        - e^{-t^2/2} \int^\infty_{-\infty} \exp \left\{ - \frac{1}{2} (x - it)^2 \right\} \> \mathrm{d}x - \sqrt{2\pi}.
    \]
    Note that $e^{-z^2/2}$ is an analytic function and $\oint_\gamma e^{-z^2/2} \> \mathrm{d}z = 0$ on closed path.
\end{proof}

\begin{theorem}[Strong Law of Large Numbers]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of i.i.d random variables, each with mean 
    $\mathbb{E}[X_i] = \mu$ and standard deviation $\sigma$, then 
    \begin{equation}
        \mathbb{P}[\Lim{n \to \infty} \ols{X}_n = \mu] = 1. 
    \end{equation}
\end{theorem}

\begin{proof}
    By Markov's inequality that 
    \[
        \mathbb{P} \left[ \frac{1}{n}|\ols{X}_n - n\mu| \geq n^{-\gamma}\right] 
        \leq \frac{\mathbb{E}[(\mfrac{\ols{X}_n}{n} - \mu)^4]}{n^{-4\gamma}} = Kn^{-2+4\gamma}.
    \]
    Define for all $\gamma \in \left(0, \frac{1}{4}\right)$, and let 
    \[
        A_n = \left\{ \frac{1}{n}|\ols{X}_n - n\mu| \geq n^{-\gamma} \right\} \Rightarrow 
        \sum_{n \geq 1} \mathbb{P}[A_n] < \infty \Rightarrow 
        \mathbb{P}[A] = 0
    \]
    by the first Borel-Cantelli lemma, where $A = \bigcap_{n \geq 1} \bigcup_{m \geq n} A_n$. But now, the event 
    $A^c$ happened if and only if 
    \[
        \exists N\, \forall n \geq N \> \left| \frac{\ols{X}_n}{n} - \mu \right| < n^{-\gamma} 
        \Rightarrow \frac{\ols{X}_n}{n} \xrightarrow[]{p} \mu.
    \]
\end{proof}

\begin{lemma}
    We can write $f(x) = \mathcal{O}(x)$ if $\frac{f(x)}{x} \to 0$ as $x \to 0$. We have 
    \begin{equation}
        \lim_{x \to 0} \left[ 1 + \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right)\right]^n = e^a \quad \forall a \in \mathbb{R}.
    \end{equation}
\end{lemma}
\begin{proof}
    Using Taylor's expansion we have
    \begin{align*}
        &f(x) = f(0) + x f'(\theta x), \quad \text{for }0 < \theta < 1\\
        &\Rightarrow f(x) = f(0) + xf'(0) + x [f'(\theta x) - f'(0)].
    \end{align*}
    If $f'(x)$ is continuous at $x=0$, then $f'(\theta x) - f'(0) = \mathcal{O}(x)$ as $x \to 0$. Now let 
    $f(x) = \ln (1 + x)$. Taking derivative we have $f'(x) = \mfrac{1}{1+x}$, which is continuous at $x=0$. From here we have
    \[
        \ln (1 + x) = \ln 1 + x + \mathcal{O}(x) \implies \ln (1 + x) = x + \mathcal{O}(x).
    \]
    Then for sufficiently large $n$, we have
    \begin{align*}
        n \ln \left[ 1 + \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right)\right] 
        &= n \left[ \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right) + \mathcal{O}\left( \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right) \right)\right]\\
        &= a + n \mathcal{O}\left(\frac{1}{n} \right) + n \mathcal{O}\left( \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right) \right)\\
        &= a + n \mathcal{O}\left(\frac{1}{n} \right) + n \mathcal{O}\left(\frac{1}{n} \right)
        \to a \quad \text{as } n \to \infty.
    \end{align*}
    Taking exponential on both sides we have
    \[
        \lim_{x \to 0} \left[ 1 + \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right)\right]^n = e^a
    \]
    for every real $a$, and now we are done.
\end{proof}
\begin{remark}
    Notice that 
    \[
        \mathcal{O} \left( \frac{1}{n}\right) = \frac{k_1}{n^2} + \frac{k_2}{n^3} + \cdots.
    \]
    and 
    \begin{align*}
        \mathcal{O}\left( \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right) \right) &= k_1 \left( \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right) \right)^2 + 
        k_2 \mathcal{O}\left( \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right) \right)^3 + \cdots\\
        &= \frac{c_1}{n^2} + \frac{c_2}{n^3} + \cdots.
    \end{align*}
    for arbitrary constants $c_1, c_2, \ldots$, this implies that $\mathcal{O}\left( \frac{a}{n} + \mathcal{O}\left(\frac{1}{n} \right) \right)$ 
    is also $\mathcal{O}\left( \frac{1}{n} \right)$.
\end{remark}

\section{Central Limit Theorem}

\begin{theorem}[Central Limit Theorem]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of i.i.d random variable whose moment generating function 
    exist in a neighborhood of 0. Let $\mathbb{E}[X_i] = \mu$ and $Var[X_i] = \sigma^2 > 0$. Define
    $\ols{X} = \mfrac{1}{n} \sum^n_{i=1} X_i$. Then 
    \begin{equation}
        Z = \frac{\sqrt{n} (\ols{X}_i - \mu)}{\sigma} \xrightarrow[]{\mathcal{D}} \mathcal{N}(0,1)
    \end{equation}
    or 
    \begin{equation}
        Z = \frac{\sum^n_{i=1} X_i - n\mu}{\sqrt{n} \sigma} \xrightarrow[]{\mathcal{D}} \mathcal{N}(0,1)
    \end{equation}
\end{theorem}

\begin{example}
    Let $\ols{X}_n$ be the sample mean from a random sampling of size 
    $n = 100$ from $\chi^2_{50}$. Compute approximate value of 
    $\mathbb{P}(49 < \ols{X} < 51)$.
\end{example}
\begin{solution}
    Because $\ols{X}$ followed Chi-squared distribution with degree of freedom 50, then the mean and variance are 
    $\mathbb{E}[X_i] = 50$ and $Var[X_i] = 2(50) = 100$. By Central Limit Theorem, 
    \begin{align*}
        \mathbb{P}(49 < \ols{X} < 51) &\simeq  
        \mathbb{P} \left[ \frac{\sqrt{100} (49 - \mathbb{E}[X_i])}{\sqrt{Var[X_i]}} < Z < \frac{\sqrt{100} (51 - \mathbb{E}[X_i])}{\sqrt{Var[X_i]}} \right]\\
        &\simeq \mathbb{P} \left[ \frac{\sqrt{100} (49 - 50)}{\sqrt{100}} < Z < \frac{\sqrt{100} (51 - 50)}{\sqrt{100}} \right]\\
        &\simeq \mathbb{P} \left[ -1 < Z < 1 \right]\\
        &\simeq \Phi(1) - \Phi(-1)\\
        &\simeq 0.84134 - 0.15866 = 0.68268.
    \end{align*} 
\end{solution}

\begin{theorem}[De Moivre-Laplace Limit Theorem]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of identically independent Bernoulli random variable with 
    parameter $p$. Such as 
    \[
        \mathbb{P}[X_n = 1] = p, \quad \mathbb{P}[X_n = 0] = 1-p \quad \text{where } 0<p<1.
    \]
    Then 
    \begin{equation}
        \Lim{n \to \infty} \mathbb{P} \left[\frac{\sum^n_{k=1} X_k - np}{\sqrt{npq}} \leq x \right] = \Phi(x).
    \end{equation}
    That is, the sum $\sum^n_{k=1} X_k \xrightarrow[]{\mathcal{D}} X \sim N(np, npq)$. Also, $\frac{\sum^n_{k=1} X_k - np}{\sqrt{npq}}$
    is \textit{asymptotically normal} with mean zero and unit variance.
\end{theorem}
\begin{proof}
    Let $S_n = \sum^n_{k=1} X_k$ and then the mean and variance are $\mathbb{E}[S_n] =np$ 
    and $Var[S_n] = \sqrt{npq}$. The characteristic function is 
    \begin{align*}
        \phi_n(t) &= \mathbb{E} \left[ \exp \left\{ \left( \frac{S_n - np}{\sqrt{npq}} \right)it \right\} \right]\\
        &= \prod_{k=1}^{n} \mathbb{E} \left[ \exp \left\{ \frac{it}{\sqrt{n}} \left( \frac{X_k - p}{\sqrt{p(1-p)}} \right)\right\} \right]\\
        &= \left[ \phi \left( \frac{t}{\sqrt{n}} \right) \right]^n
    \end{align*}
\end{proof}

\subsubsection*{Alternative proof}
\begin{proof}
    Since each $X_k$'s are identically independent Bernoulli random variable with mean $\mathbb{E}X_k = p$ and 
    $Var(X_k) = pq \leq \mfrac{1}{4} < \infty$. By Lindeberg-L\'evy CLT theorem,
    \begin{equation}
        \frac{\sqrt{n} (\ols{X}_n - p)}{\sqrt{pq}} \xrightarrow[]{\mathcal{D}} X \sim N(0,1).
    \end{equation}
    Let the sum be $S_n = \sum^n_{k=1} X_k$, the probability mass function of $S_n$ is given by 
    \begin{align*}
        p(x) &= \frac{n!}{x!(n-x)!}p^x q^{n-x}\\
        &\approx \frac{\sqrt{2\pi}e^{-n}n^{n+\frac{1}{2}} p^x q^{n-x}}{\sqrt{2\pi}e^{-x} x^{x+\frac{1}{2}}\, \sqrt{2\pi}e^{-(n-x)} (n-x)^{(n-x) + \frac{1}{2}}}\\
        &\approx \frac{1}{\sqrt{2\pi}\, \sqrt{npq}} \left(\frac{np}{x}\right)^{x+\frac{1}{2}} \left(\frac{nq}{n-x}\right)^{n-x+\frac{1}{2}}
    \end{align*}
    Now let 
    \[
        \delta = \frac{x - np}{\sqrt{npq}}
    \]
    where $\delta > 0$. 
    \begin{align*}
        &\Rightarrow x = np + \delta \sqrt{npq}\\
        &\Rightarrow n - x = nq - \delta \sqrt{npq}\\
    \end{align*}
    Hence, the Binomial density function can be written as 
    \[
        f(x) = \frac{1}{\sqrt{2\pi} \, \sqrt{npq}} \left( 1 + \delta \sqrt{\frac{q}{np}}\right)^{-x-\frac{1}{2}} \left( 1 - \delta \sqrt{\frac{p}{nq}}\right)^{-(n-x) - \frac{1}{2}}
    \]
    Taking logarithm on both sides, 
    \[
        \ln f(x) = \ln \frac{1}{\sqrt{2\pi} \, \sqrt{npq}} - \left(x + \frac{1}{2}\right)\ln \left( 1 + \delta \sqrt{\frac{q}{np}}\right) - \left( n - x + \frac{1}{2} \right)
        \ln \left( 1 - \delta \sqrt{\frac{p}{nq}}\right)
    \]
    To make it easier, observe that $\ln \frac{1}{\sqrt{2\pi} \, \sqrt{npq}}$ is a constant. We let $c = \ln \frac{1}{\sqrt{2\pi} \, \sqrt{npq}}$ and continue evaluate 
    this algebraic expression.
    \begin{align*}
        \ln f(x) &= c - \left(np + \delta \sqrt{npq} + \frac{1}{2} \right)
        \left[ \delta \sqrt{\frac{q}{np}} - \delta^2 \frac{q}{2np} + \delta^3 \frac{q^{3/2}}{3(np)^{3/2}} + \cdots \right]\\
        &\quad + \left(nq - \delta \sqrt{npq} + \frac{1}{2} \right)\left[ \delta \sqrt{\frac{q}{np}} - \delta^2 \frac{q}{2np} + \delta^3 \frac{q^{3/2}}{3(np)^{3/2}} + \cdots \right]\\
        &=c + \left[ -\delta \sqrt{npq} + \delta \sqrt{npq} \right] 
        + \left[-(\delta^2q + \delta^2p) + \left(\delta^2 \frac{q}{2} + \delta^2 \frac{p}{2}\right)\right]\\
        &\quad + \left[ \left(- \frac{\delta}{2} \sqrt{\frac{q}{np}} + \frac{\delta}{2} \sqrt{\frac{p}{nq}} \right) 
            + \left( \delta^3 \frac{q^{3/2}}{2(np)^{3/2}} - \delta^3 \frac{p^{3/2}}{2(nq)^{3/2}} \right) 
        + \left( -\delta^3 \frac{q^{3/2}}{3(np)^{3/2}} + \delta^3 \frac{p^{3/2}}{3(nq)^{3/2}} \right) \right]\\
        &\quad + \left[ \left(\frac{\delta^2q}{4np} + \frac{\delta^2p}{4nq} \right) - \left(\frac{\delta^2q^2}{3np} + \frac{\delta^2p^2}{3nq} \right) 
        + \left(\frac{\delta^4q^2}{4np} + \frac{\delta^4p^2}{4nq} \right) \right] + \cdots\\
    \end{align*}
    Assuming that $\frac{|\delta|}{n^{1/\delta}} \to 0$ for large $n$. Then
    \[
        \frac{\delta}{\sqrt{n}} \to 0, \quad \frac{\delta^2}{n} \to 0, \quad \frac{\delta^3}{\sqrt{n}} \to 0, \quad
        \frac{\delta^4}{n^{2/3}} \to 0 \quad \text{as } n \to \infty.
    \]
    So that $\ln f(x)$ is approximate to $c - \frac{1}{2}\delta^2$. Hence 
    \[
        f(x) = e^{c - \frac{1}{2}\delta^2} = \frac{1}{\sqrt{2\pi} \, \sqrt{npq}} \exp \left\{ -\frac{1}{2} \left(\frac{x -np}{\sqrt{npq}}\right)^2 \right\} \sim N(np, npq).
    \]
\end{proof}

\begin{theorem}[Lyapunov Limit Theorem]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of identically independent random variables with mean 
    $\mathbb{E}[X_n] = \mu_n$ and variance $Var(X_n) = \sigma^2_n$, and
    \[
        \mathbb{E}|X_n - \mu_n|^{2+\delta} < \infty \quad \text{for some } \delta > 0.
    \]
    Define the sum of sequence as
    \[
        S_n = \sum^n_{k=1} X_k.
    \]
    Then
    \begin{equation}
        \mathbb{P} \left[ \frac{S_n - \sum^n_{k=1} \mu_k}{\sqrt{\sum^n_{k=1} \sigma^2_k}}\right] \to \Phi(x) \sim N(0,1) \quad \text{as } n \to \infty.
    \end{equation}
    Provided that the Lyapunov condition is satisfied, namely
    \begin{equation}
        \lim_{n \to \infty} \frac{\sum^n_{k=1} \mathbb{E} |X_k - \mu_k|^{2+\delta}}{\left( \sum^n_{k=1} \sigma^2_k \right)^{1+\delta/2}} = 0.
    \end{equation}
\end{theorem}
The cases when $\delta > 1$ can be reduced to the case $\delta = 1$, thus it is enough to consider that $0< \delta <1$.
\begin{proof}
    We use the following bound to verify Lyapunov's condition:
    \[
        \frac{1}{\widehat{\sigma^2_n}} \sum^{r_n}_{k=1} \int_{|X_{nk}| > \epsilon \widehat{\sigma}_n} X^2_{nk} \> \mathrm{d}P 
        \leq \frac{1}{\epsilon^{\delta} \widehat{\sigma}^{2+\delta}_n} \sum^{r_n}_{k=1} \int_{|X_{nk}| > \epsilon \widehat{\sigma}_n} |X_{nk}|^{2+\delta} \> \mathrm{d}P 
        \leq \frac{1}{\epsilon^{\delta} \widehat{\sigma}^{2+\delta}_n} \sum^{n}_{k=1} \mathbb{E} |X_{nk}|^{2+\delta}.
    \]
\end{proof}
\begin{corollary}
    Suppose $X_k$ are independent with mean zero, variance $\sigma^2$ and that $\sup_k \mathbb{E}|X_k|^{2+\delta} < \infty$. Then 
    \[
        \frac{S_n}{\sigma \sqrt{n} } \xrightarrow[]{\mathcal{D}} \sigma N(0,1).
    \]
\end{corollary}
\begin{proof}
    Let $C = \sup_k \mathbb{E} |X_k|^{2+\delta}$. Then $s_n = \sqrt{n}$ and 
    \[
        \frac{1}{s^{2+\delta}_n} \sum^n_{k=1} \mathbb{E}|X_k|^{2+\delta} \leq \frac{nC}{n^{1+\delta/2}} = \frac{C}{n^{\delta/2}} \to 0.
    \]
    as $n \to \infty$. So Lyapunov's condition is satisfied, and we are done.
\end{proof}

\section*{Tutorials}

\begin{mdframed}
    \vspace{-0.25cm}
    \hspace{-0.25cm}
    \begin{Exercise}
        Let $X_1, X_2, \ldots, X_{2020}$ be a random sample of size $2020$ 
        from a Poisson distribution with density function
        \[
            f_{X_i}(x) = \frac{e^{-\lambda} \lambda^x}{x!}, \quad x = 0,1,2,\ldots,\infty.
        \]
        What is the distribution of $2020 \ols{X}$?
    \end{Exercise}
\end{mdframed}