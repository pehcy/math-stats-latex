\chapter{Limiting Distribution}

\section{Covergence in distribution}

Convergence in distribution state that 
the sequence of random variables $X_1, X_2, \ldots, X_n$ 
converges to some distribution $X$ when $n$ goes to infinity.
It does not require any dependence between the $X_n$ and $X$. Convergence in distribution is 
consider as the weakest type of convergence.

\begin{definition}[Covergence in distribution]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of random variable and let $X$ be a random variable. Let $F_{X_n}$ and 
    $F_X$ be the cdf of $X_n$ and $X$ respectively. And let $C(F)$ be the set of all continuous points of
    $F$. We say that $X_n$ converges in distribution to $X$ if 
    \begin{equation}
        \Lim{n \to \infty} F_{X_n}(x) = F_X(x)
    \end{equation}
    for all $x \in C(F)$. We denote $X_n \xrightarrow[]{d} X$. 
\end{definition}

\begin{example}
    Let $X_1, X_2, X_3, \ldots$ be a sequence of random variable such that 
    \[
        X_n \sim \text{Geom}(\lambda /n), \quad \forall n = 1,2,3,\ldots
    \]
    where $\lambda > 0$ is a constant. Define a new sequence $Y_n$ as 
    \[
        Y_n = \frac{1}{n}X_n, \quad \forall n = 1,2,3,\ldots
    \]
    Show that $Y_n$ converges in distribution to $\text{Exp}(\lambda)$.
\end{example}

\begin{solution}
    The cdf of $Y_n$ is 
    \begin{align*}
        F_{Y_n}(y) &= \mathbb{P}\left[ \frac{1}{n}X_n \leq y \right]\\
        &= \mathbb{P}\left[ X_n \leq ny \right]\\
        &= 1 - \left( 1- \frac{\lambda}{n} \right)^{\text{floor}(ny)}.
    \end{align*}
    and taking limit for $n \to +\infty$ we have 
    \begin{align*}
        \Lim{n \to \infty} \left( 1 - \left( 1- \frac{\lambda}{n} \right)^{\lfloor ny \rfloor } \right) 
        &= 1 - \Lim{n \to \infty} \left( 1 + \frac{1}{\left(-\mfrac{n}{\lambda} \right)} \right)^{-n(-y)}\\
        &= 1 - \Lim{n \to \infty} \left( 1 + \frac{1}{\left(-\mfrac{n}{\lambda}\right)} \right)^{-\mfrac{n}{\lambda}(-\lambda y)}\\
        &= 1 - e^{-\lambda y}
    \end{align*}
    which is the cdf of exponential distribution with parameter $\lambda$.
\end{solution}

\begin{example}
    Let $\{X_n \}_{n \geq 1}$ be a sequence of random variables with probability mass function 
    \[
        f_{X_n}(x) = \mathbb{P}[X_n = x] = \begin{cases}
            1 & \text{if } x = 2 + \mfrac{1}{n}\\
            0 & \text{otherwise}.
        \end{cases}
    \]
    Find the limiting distribution of $X_n$.
\end{example}

\subsection{Almost sure converges}

We said that $X_n$ converges to $X$ \textbf{almost surely} if the probability that the sequence $X_n(s)$ 
converges to $X(s)$ is equal to 1.

\begin{example}
    Consider the sample space $S = [0,1]$ with uniform probability distribution, for instance,
    \[
        \mathbb{P}([a,b]) = b - a \quad \forall 0 \leq a \leq b \leq 1.
    \]
    Define the sequence $\{X_n, n=1,2,3,\ldots\}$ as 
    \[
        X_n(s) = \frac{n}{n+1}s + (1-s)^n.
    \]
    Also, define the random variable $X$ on the sample space as $X(s) = s$. Show that $X_n$ 
    \textit{almost sure} converges to $X$.
\end{example}
\begin{solution}
    For any $s \in [0,1]$, taking the limit when $n >> \infty$ we have 
    \begin{align*}
        \Lim{n \to \infty}X_n(s) &= \Lim{n \to \infty} \left[ \frac{n}{n+1}s + (1 - s)^n \right]\\
        &= \Lim{n \to \infty} \frac{n}{n+1}s + \Lim{n \to \infty} (1 - s)^n\\
        &= 1\cdot s + 0\\
        &= s = X(s).
    \end{align*}
    However, if $s=0$ then 
    \[
        \Lim{n \to \infty}X_n(0) = \Lim{n \to \infty} \left[ \frac{n}{n+1}(0) + (1-0)^n\right] = 1.
    \]
    Thus, we conclude that $\Lim{n \to \infty} X_n(s) = X(s)$ for all $s$ lies between 0 and 1.
    And because $\mathbb{P}([0,1]) = 1$, we conclude that
    \[
        X_n \xrightarrow[]{a.s.} X
    \]
    and we are done.
\end{solution}

\section{Law of Large Numbers}

In this section we will discuss the Weak and Strong Law of Large Numbers. The Law of Large 
Numbers are consider as a form of convergence in probability.

We will first state the Weak Law of Large Numbers (WLLN),

\begin{theorem}[Weak Law of Large Numbers (WLLN)]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of i.i.d random variables, each with mean 
    $\mathbb{E}[X_i] = \mu$ and standard deviation $\sigma$, we define 
    \[
        \ols{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n}.
    \]
    The Weak Law of Large Numbers (WLLN) states that for all $\epsilon > 0$, we have 
    \begin{equation}
        \Lim{n \to \infty} \mathbb{P}[\, |\ols{X}_n - \mu| > \epsilon] = 0.
    \end{equation}
\end{theorem}

\begin{proof}
    Suppose that $Var[X_i] = \sigma^2 > 0$ for finite $i$. Since 
    $X_1, X_2, \ldots, X_n$ are identically independent, there is no 
    correlation between them, thus
    \begin{align*}
        Var[\ols{X}_n] &= Var \left[ \frac{X_1 + X_2 + \cdots + X_n}{n} \right]\\
        &= \frac{1}{n^2} Var[X_1 + X_2 + \cdots + X_n]\\
        &= \frac{1}{n^2} [Var X_1 + Var X_2 + \cdots + Var X_n]\\
        &= \frac{1}{n^2} (\underbrace{\sigma^2 + \sigma^2 + \cdots + \sigma^2}_{n \text{ times}})\\
        &= \frac{n\sigma^2}{n^2}
        = \frac{\sigma^2}{n}.
    \end{align*}

    Notice that the mean of each $X_i$ in the sequence is also equal to the mean of the sample average, 
    said $\mathbb{E}[X_i] = \mu$. We can now apply Chebyshev's inequality on $\ols{X}_n$ to get, for all 
    $\epsilon > 0$, 
    \[
        \mathbb{P}[\, |\ols{X}_n - \mu| > \epsilon] \leq \frac{\sigma^2}{n\epsilon^2}.
    \]
    So that 
\end{proof}

\begin{theorem}[Strong Law of Large Numbers]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of i.i.d random variables, each with mean 
    $\mathbb{E}[X_i] = \mu$ and standard deviation $\sigma$, then 
    \begin{equation}
        \mathbb{P}[\Lim{n \to \infty} \ols{X}_n = \mu] = 1. 
    \end{equation}
\end{theorem}

\begin{proof}
    By Markov's inequality that 
    \[
        \mathbb{P} \left[ \frac{1}{n}|\ols{X}_n - n\mu| \geq n^{-\gamma}\right] 
        \leq \frac{\mathbb{E}[(\mfrac{\ols{X}_n}{n} - \mu)^4]}{n^{-4\gamma}} = Kn^{-2+4\gamma}.
    \]
    Define for all $\gamma \in \left(0, \frac{1}{4}\right)$, and let 
    \[
        A_n = \left\{ \frac{1}{n}|\ols{X}_n - n\mu| \geq n^{-\gamma} \right\} \Rightarrow 
        \sum_{n \geq 1} \mathbb{P}[A_n] < \infty \Rightarrow 
        \mathbb{P}[A] = 0
    \]
    by the first Borel-Cantelli lemma, where $A = \bigcap_{n \geq 1} \bigcup_{m \geq n} A_n$. But now, the event 
    $A^c$ happened if and only if 
    \[
        \exists N\, \forall n \geq N \> \left| \frac{\ols{X}_n}{n} - \mu \right| < n^{-\gamma} 
        \Rightarrow \frac{\ols{X}_n}{n} \xrightarrow[]{p} \mu.
    \]
\end{proof}

\begin{theorem}[Central Limit Theorem]
    Let $\{X_n\}_{n \geq 1}$ be a sequence of i.i.d random variable whose memoment generating function 
    exist in a neighborhood of 0. Let $\mathbb{E}[X_i] = \mu$ and $Var[X_i] = \sigma^2 > 0$. Define
    $\ols{X} = \mfrac{1}{n} \sum^n_{i=1} X_i$. Then 
    \begin{equation}
        Z = \frac{\sqrt{n} (\ols{X}_i - \mu)}{\sigma} \xrightarrow[]{d} \mathcal{N}(0,1)
    \end{equation}
    or 
    \begin{equation}
        Z = \frac{\sum^n_{i=1} X_i - n\mu}{\sqrt{n} \sigma} \xrightarrow[]{d} \mathcal{N}(0,1)
    \end{equation}
\end{theorem}

\begin{example}
    Let $\ols{X}_n$ be the sample mean from a random sampling of size 
    $n = 100$ from $\chi^2_{50}$. Compute approximate value of 
    $\mathbb{P}(49 < \ols{X} < 51)$.
\end{example}
\begin{solution}
    Because $\ols{X}$ followed Chi-squared distribution with degree of freedom 50, then the mean and variance are 
    $\mathbb{E}[X_i] = 50$ and $Var[X_i] = 2(50) = 100$. By Central Limit Theorem, 
    \begin{align*}
        \mathbb{P}(49 < \ols{X} < 51) &\simeq  
        \mathbb{P} \left[ \frac{\sqrt{100} (49 - \mathbb{E}[X_i])}{\sqrt{Var[X_i]}} < Z < \frac{\sqrt{100} (51 - \mathbb{E}[X_i])}{\sqrt{Var[X_i]}} \right]\\
        &\simeq \mathbb{P} \left[ \frac{\sqrt{100} (49 - 50)}{\sqrt{100}} < Z < \frac{\sqrt{100} (51 - 50)}{\sqrt{100}} \right]\\
        &\simeq \mathbb{P} \left[ -1 < Z < 1 \right]\\
        &\simeq \Phi(1) - \Phi(-1)\\
        &\simeq 0.84134 - 0.15866 = 0.68268.
    \end{align*} 
\end{solution}

\begin{theorem}[Slutsky's Theorem]
    If $X_n$ \textit{converges in distribution} to a random variable $X$, and $Y_n$ \textit{converges in probability} to a constant $c$, then 
    \begin{itemize}
        \item[$\bullet$] $Y_nX_n \xrightarrow[]{d} cX$
        \item[$\bullet$] $X_n + Y_n \xrightarrow[]{d} X + c$.
    \end{itemize}
\end{theorem}

\begin{proof}
    We will prove both parts of Slutsky's theorem.
    
    \textbf{Part 1:} We want to show that $Y_nX_n \xrightarrow{d} cX$.
    
    Since $Y_n \xrightarrow{p} c$, for any $\epsilon > 0$, we have $\mathbb{P}[|Y_n - c| > \epsilon] \to 0$ as $n \to \infty$.
    This implies that $Y_n$ is bounded in probability, and we can write $Y_n = c + (Y_n - c)$ where $(Y_n - c) \xrightarrow{p} 0$.
    
    Now, $Y_nX_n = cX_n + (Y_n - c)X_n$. Since $X_n \xrightarrow{d} X$ and multiplication by the constant $c$ is a continuous operation, we have $cX_n \xrightarrow{d} cX$.
    
    For the second term, we need to show that $(Y_n - c)X_n \xrightarrow{p} 0$. Since $Y_n - c \xrightarrow{p} 0$ and $X_n$ is bounded in probability (as it converges in distribution), their product converges to 0 in probability.
    
    By Slutsky's theorem for sums (which we prove next), we get $Y_nX_n = cX_n + (Y_n - c)X_n \xrightarrow{d} cX + 0 = cX$.
    
    \textbf{Part 2:} We want to show that $X_n + Y_n \xrightarrow{d} X + c$.
    
    We use characteristic functions. Let $\phi_{X_n}(t)$, $\phi_X(t)$, and $\phi_{Y_n}(t)$ denote the characteristic functions of $X_n$, $X$, and $Y_n$, respectively.
    
    The characteristic function of $X_n + Y_n$ is given by:
    \begin{align*}
        \phi_{X_n + Y_n}(t) &= \mathbb{E}[e^{it(X_n + Y_n)}] = \mathbb{E}[e^{itX_n}e^{itY_n}]
    \end{align*}
    
    Since $Y_n \xrightarrow{p} c$, we have $e^{itY_n} \xrightarrow{p} e^{itc}$ by the continuous mapping theorem.
    
    Using the fact that $X_n \xrightarrow{d} X$ implies $\phi_{X_n}(t) \to \phi_X(t)$, and that convergence in probability preserves the limit of expectations for bounded random variables, we get:
    \begin{align*}
        \lim_{n \to \infty} \phi_{X_n + Y_n}(t) &= \lim_{n \to \infty} \mathbb{E}[e^{itX_n}e^{itY_n}] \\
        &= \mathbb{E}[e^{itX}] \cdot e^{itc} \\
        &= \phi_X(t) \cdot e^{itc} \\
        &= \phi_{X + c}(t)
    \end{align*}
    
    Since the characteristic function of $X_n + Y_n$ converges pointwise to the characteristic function of $X + c$, we conclude that $X_n + Y_n \xrightarrow{d} X + c$.
\end{proof}

\begin{example}
    If the random variable $X \sim \text{Gamma}(\mu, 1)$, show that 
    \[
        \frac{\sqrt{n}(\ols{X}_n - \mu)}{\sqrt{\ols{X}_n}} \xrightarrow[]{d} \mathcal{N}(0,1)
    \]
\end{example}
\begin{solution}
    Slutsky's theorem stated that If $X_n$ converges in distribution to a random variable 
    $X$ and if $Y_n$ converges in probability to a constant $c$. Then $X_n / Y_n$
    converges in distribution to $X/c$. By the central limit theorem we have 
    \[
        \sqrt{n}(\ols{X}_n - \mu) \xrightarrow[]{d} \mathcal{N}(0, Var[X_i]).
    \]
    and in this case $\mathbb{E}X_i = Var[X_i] = \mu$, thus we obtained
    \[
        \sqrt{n}(\ols{X}_n - \mu) \xrightarrow[]{d} \mathcal{N}(0, \mu).
    \]
    Replacing the theorem denominator $Y_n$ with $\ols{X}_n$, which $\ols{X}_n$ converges to constant $\mu$ 
    in probability. Hence
    \[
        \frac{\sqrt{n}(\ols{X}_n - \mu)}{\sqrt{\ols{X}_n}} \xrightarrow[]{d} \mathcal{N}\left(\frac{0}{\mu}, \frac{\mu}{\mu}\right)
        \Longrightarrow \frac{\sqrt{n}(\ols{X}_n - \mu)}{\sqrt{\ols{X}_n}} \xrightarrow[]{d} \mathcal{N}(0,1)
    \]
    and we are done with the proof.
\end{solution}

\begin{theorem}
    If the random variable $X_n$ converges to constant $c$ in probability, then 
    \begin{equation}
        \sqrt{X_n} \xrightarrow[]{p} \sqrt{c},\quad c > 0. 
    \end{equation}
\end{theorem}

\begin{theorem}
    If the random variable $X_n$ converges to constant $c$ in probability, and 
    $Y_n$ converges to constant $d$ in probability, then 
    \begin{itemize}
        \item[$\bullet$] $aX_n + bY_n \xrightarrow[]{p} ac + bd$.
        \item[$\bullet$] $X_n Y_n \xrightarrow[]{p} cd$.
        \item[$\bullet$] $\mfrac{1}{X_n} \xrightarrow[]{p} \mfrac{1}{c}$ for all $c \neq 0$.
    \end{itemize}
\end{theorem}

\begin{proof}
    We will prove each part of the theorem.
    
    \textbf{Part 1:} We want to show that $aX_n + bY_n \xrightarrow{p} ac + bd$.
    
    For any $\epsilon > 0$, we have:
    \begin{align*}
        |aX_n + bY_n - (ac + bd)| &= |a(X_n - c) + b(Y_n - d)| \\
        &\leq |a||X_n - c| + |b||Y_n - d|
    \end{align*}
    
    By the triangle inequality, for any $\delta > 0$:
    \begin{align*}
        &\mathbb{P}[|aX_n + bY_n - (ac + bd)| > \epsilon] \\
        &\leq \mathbb{P}[|a||X_n - c| + |b||Y_n - d| > \epsilon] \\
        &\leq \mathbb{P}[|a||X_n - c| > \epsilon/2] + \mathbb{P}[|b||Y_n - d| > \epsilon/2] \\
        &= \mathbb{P}[|X_n - c| > \mfrac{\epsilon}{2a}] + \mathbb{P}[|Y_n - d| > \mfrac{\epsilon}{2b}], \quad \forall a, b > 0
    \end{align*}
    
    Since $X_n \xrightarrow{p} c$ and $Y_n \xrightarrow{p} d$, both terms on the right approach 0 as $n \to \infty$.
    
    \textbf{Part 2:} We want to show that $X_n Y_n \xrightarrow{p} cd$.
    
    We can write:
    \begin{align*}
        X_n Y_n - cd &= X_n Y_n - cY_n + cY_n - cd \\
        &= Y_n(X_n - c) + c(Y_n - d)
    \end{align*}
    
    Since $Y_n \xrightarrow{p} d$, the sequence $\{Y_n\}$ is bounded in probability. That is, for any $\delta > 0$, there exists $M > 0$ such that $\mathbb{P}[|Y_n| > M] < \delta$ for all $n$ sufficiently large.
    
    For any $\epsilon > 0$:
    \begin{align*}
        |X_n Y_n - cd| &= |Y_n(X_n - c) + c(Y_n - d)| \\
        &\leq |Y_n||X_n - c| + |c||Y_n - d|
    \end{align*}
    
    Given $\epsilon > 0$, choose $\delta > 0$ such that:
    \begin{align*}
        &\mathbb{P}[|X_n Y_n - cd| > \epsilon] \\
        &\leq \mathbb{P}[|Y_n||X_n - c| + |c||Y_n - d| > \epsilon] \\
        &\leq \mathbb{P}[|Y_n||X_n - c| > \epsilon/2] + \mathbb{P}[|c||Y_n - d| > \epsilon/2]
    \end{align*}
    
    For the first term, using the boundedness of $Y_n$ and convergence of $X_n$, and for the second term using convergence of $Y_n$, both approach 0 as $n \to \infty$.
    
    \textbf{Part 3:} We want to show that $\frac{1}{X_n} \xrightarrow{p} \frac{1}{c}$ for $c \neq 0$.
    
    Since $c \neq 0$, there exists $\delta > 0$ such that $|c| > \delta > 0$. Because $X_n \xrightarrow{p} c$, for any $\epsilon > 0$, we have $\mathbb{P}[|X_n - c| > \epsilon] \to 0$.
    
    In particular, $\mathbb{P}[|X_n - c| > \delta/2] \to 0$, which implies $\mathbb{P}[|X_n| > \delta/2] \to 1$. This means $X_n$ is bounded away from 0 in probability.
    
    Now, for any $\epsilon > 0$:
    \begin{align*}
        \left|\frac{1}{X_n} - \frac{1}{c}\right| &= \left|\frac{c - X_n}{X_n c}\right| = \frac{|X_n - c|}{|X_n||c|}
    \end{align*}
    
    On the event $\{|X_n| > \delta/2\}$, we have:
    \begin{align*}
        \left|\frac{1}{X_n} - \frac{1}{c}\right| \leq \frac{2|X_n - c|}{\delta|c|}
    \end{align*}
    
    Therefore:
    \begin{align*}
        \mathbb{P}\left[\left|\frac{1}{X_n} - \frac{1}{c}\right| > \epsilon\right] &\leq \mathbb{P}[|X_n| \leq \delta/2] + \mathbb{P}\left[\frac{2|X_n - c|}{\delta|c|} > \epsilon, |X_n| > \delta/2\right] \\
        &\leq \mathbb{P}[|X_n| \leq \delta/2] + \mathbb{P}\left[|X_n - c| > \frac{\epsilon\delta|c|}{2}\right]
    \end{align*}
    
    As $n \to \infty$, both terms approach 0, completing the proof.
\end{proof}

\section{Order Statistics}

We can ordering the observed random variables based on their magnitudes or ranking. These ordered variables 
are known as \textbf{order statistics}.

Consider $X_1, X_2, \ldots, X_n$ are independent countinuous random variables with cdf $F_X(y)$ and mass function 
$f_X(y)$. We ordered them into order statistics $X_{(1)}, X_{(2)}, \ldots, X_{(n)}$ such that 
\[
X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(n)}.
\]
In this notion, the maximum random variable is 
\[
    X_{(n)} = \max \{ X_1, X_2, \ldots, X_n \}
\]
and the minimum random variable is 
\[
    X_{(1)} = \min \{ X_1, X_2, \ldots, X_n \}
\]
$X_{(n)}$ is the largest among $X_1, X_2, \ldots, X_n$, the event $X_{(n)} \leq y$ will happened only if each 
$X_i \leq y$. Then the joint probability is
\begin{equation*}
    G_{(n)}(y) = \mathbb{P}[X_{(n)} \leq y] = \mathbb{P}[X_1 \leq y,\> X_2 \leq y,\> \ldots,\> X_n \leq y] = 
    \prod_{i=1}^{n} \mathbb{P}[X_i \leq y]. \label{eq:l1.0} \tag{{\color{red} $\heartsuit$}}
\end{equation*}
Because $\mathbb{P}[X_i \leq y] = F_X(y)$ for all $i=1,2,\ldots,n$. It follows that 
\begin{equation*}
    \eqref{eq:l1.0} \> \Rightarrow \> \mathbb{P}[X_1 \leq y]\,\mathbb{P}[X_2 \leq y]\,\cdots\, \mathbb{P}[X_n \leq y]
    = [F_X(y)]^n.
\end{equation*}
Now letting $g_{(n)}$ denote the density function of $Y_{(n)}$, we see that, on taking derivative on $G_{(n)}$ with respect 
to $y$.

\begin{align*}
    g_{(n)}(y) &= \odv{}{y}[F_X(y)]^n \\
    &= n[F(y)]^{n-1}\, \odv{}{y}F_X(y) & \text{By Chain rule of derivative}\\
    &= n[F(y)]^{n-1}\, f_X(y)
\end{align*}

Now we get the maximum variable. For the minimum variable $X_{(1)}$ can be found using the 
similar way. The cdf of $X_{(1)}$ is 
\[
    F_{(1)}(y) = \mathbb{P}[X_{(1)} \leq y ] = 1 - \mathbb{P}[X_{(1)} > y].
\]
Since $X_{(1)}$ is the minimum of $X_1, X_2, \ldots, X_n$, and the event $Y_i > y$ can be occurs for $i=1,2,3,\ldots,n$. 
It other words, any $X_i$ in $X_1, X_2, \ldots, X_n$ can be the minimum variable. Hence
\begin{align*}
    F_{(1)}(y) &= \mathbb{P}[X_{(1)} \leq y] = 1 - 1 - \mathbb{P}[X_{(1)} > y]\\
    &= 1 - \mathbb{P}[X_1 > y,\> X_2 > y, \> \ldots \> X_n > y]\\
    &= 1 - \mathbb{P}[X_1 > y]\, \mathbb{P}[X_2 > y] \, \ldots \> \mathbb{P}[X_n > y]\\
    &= 1 - [1 - F_X(y)]^n.
\end{align*}

% Diagram: Order Statistics (inserted by Copilot)
\begin{center}
\begin{tikzpicture}[>=Stealth,thick,scale=1.1]
    % Axis
    \draw[->] (0,0) -- (10,0) node[right] {$y$};
    % Ticks and labels
    \foreach \x/\lbl in {0.7/\small y_1, 1.5/\small y_2, 2.7/\cdots, 3.5/\small y_{k-1}, 4.5/\small y_k, 5.5/\small y_{k+1}, 6.7/\cdots, 8.2/\small y_n} {
        \draw[thick] (\x,0.15) -- (\x,-0.15);
        \node[below] at (\x,-0.15) {$\lbl$};
    }
    % Highlight line 
    \draw [ultra thick, red] (0.7,0.01) -- (3.5,0.01);
    \draw [ultra thick, cyan] (5.5,0.01) -- (8.2,0.01);
    % Braces
    \draw [draw=red,decorate,decoration={brace,amplitude=7pt,mirror}] (0.7,-0.6) -- (3.5,-0.6) node[midway,below=6pt] {};
    \draw [draw=cyan,decorate,decoration={brace,amplitude=7pt,mirror}] (5.5,-0.6) -- (8.2,-0.6) node[midway,below=6pt] {};
    \draw [<-] (4.5,-0.6)--++(-90:1) node[below]{$f_X(y_k)$};
    % Red boxes
    \node[fill=white] (A) at (2.1,-1.1) {{\color{red} $P(X\leq y_k)$}};

    \node[fill=white] (C) at (6.85,-1.1) {$P(X>y_k)$};
    
    % Big red box for orderings
    \node[draw=cyan,thick,rounded corners,fill=white,align=left,anchor=west] (D) at (8.5,1.5) {\begin{tabular}{l}
        \# of possible orderings \\ 
        $\displaystyle \frac{n!}{(k-1)!\,1!\,(n-k)!}$
    \end{tabular}};
    % Arrow to big box
    \draw[cyan,->,thick] (6.85,0.1) .. controls (7.5,0.7) .. (D.west);
\end{tikzpicture}
\end{center}

\begin{theorem}[k-th order statistics]
    Let $X_1, X_2, \ldots, X_n$ be i.i.d continuous random variable with common cdf 
    $F_X(y)$ and common density function $f_X(y)$. Let $X_{(k)}$ denote the $k$-th order Statistics, 
    then the density function of $X_{(k)}$ is
    \begin{equation}
    g_{(n)}(y) = \frac{n!}{(k-1)!(n-k)!}[F_{X}(y)]^{k-1}[1 - F_{X}(y)]^{n-k}\,f_X(y), \quad 
    -\infty < y < \infty.
    \end{equation}
\end{theorem}

\begin{example}
    Let $Y \sim \text{Uniform}(0, \theta)$ be the waiting time of bus arrival. A 
    random samples of size $n=5$ is taken. Then,
    \begin{enumerate}
        \item Find the distribution of minimum variable.
        \item Find the probability that $Y_{(3)}$ is less than $\mfrac{2}{3}\theta$.
        \item Suppose that the waiting time for bus arrival is uniformly distributed on 0 to 15 minutes, 
            find $\mathbb{P}[Y_{(5)} < 10]$.
    \end{enumerate}
\end{example}
\begin{solution}
    \begin{enumerate}
        \item The density of $X_{(1)}$ is
    \begin{align*}
        Y_{(1)} \sim g_{(1)}(y) &= \frac{5!}{(1-1)!\, (5-1)!} [F_Y(y)]^{1-1} [1 - F_Y(y)]^{5-1}\, f_Y(y)\\
        &= \frac{5!}{0!\> 4!}[1 - F_Y(y)]^4\, f_Y(y)\\
        &= 5 \left(1 - \frac{y}{\theta} \right)^4 \, \left(\frac{1}{\theta} \right)\\
        &= \frac{5(\theta - y)^4}{\theta^5}.
    \end{align*}
    Hence compute the mean of $X_{(1)}$, 
    \begin{align*}
        \mathbb{E}[Y_{(1)}] &= \int_{0}^{\theta} y\left[ \frac{5(\theta - y)^4}{\theta^5} \right] \mathrm{d}y =
        \int_{0}^{\theta} \frac{5y(\theta - y)^4}{\theta^5} \mathrm{d}y \label{eq:l2.0} \tag{{\color{gray} $\clubsuit$}}
    \end{align*}
    using the substitution method and letting $u = \theta - y$, and for that 
    \[
        y = \theta - u \Longrightarrow -du = dy
    \]
    substitute back into \eqref{eq:l2.0} and we have 
    \begin{align*}
        \eqref{eq:l2.0} = \int^\theta_0 \frac{5(\theta - u)u^4}{\theta^5}\> (-\mathrm{d}u)
        &= -\frac{1}{\theta^5} \int^\theta_0 (5\theta u^4 - u^5)\> \mathrm{d}u\\
        &= -\frac{1}{\theta^5} \left[ \theta u^5 - \frac{1}{6}\theta^6 \right]^{u=\theta}_{u=0}\\
        &= -\frac{1}{\theta^5} \left[ 0 - \frac{1}{6}\theta^6 \right]\\
        &= \frac{\theta}{6} = \mathbb{E}[Y_{(1)}].
    \end{align*}

    \item First we need to find the probability density function of $Y_{(3)}$, that is,
    \begin{align*}
        Y_{(3)} \sim g_{(3)}(y) &= \frac{5!}{(3-1)!\, (5-3)!} [F_Y(y)]^{3-1} [1 - F_Y(y)]^{5-3}\, f_Y(y)\\
        &= 30 \left( \frac{y}{\theta} \right)^2 \left(1 - \frac{y}{\theta} \right)^2\, \frac{1}{\theta}, \quad 0 < y < \theta.
    \end{align*}

    Compute the probability on which that $Y_{(3)}$ is smaller that $\mfrac{2\theta}{3}$.
    \begin{align*}
        \mathbb{P}[Y_{(3)} < \mfrac{2}{3}\theta] &= \int_{0}^{\mfrac{2}{3}\theta} 30 \left( \frac{y}{\theta} \right)^2 \left(1 - \frac{y}{\theta} \right)^2\, \frac{1}{\theta} \> \mathrm{d}y\\
        &= \frac{30}{\theta^5} \int_{0}^{\mfrac{2}{3}\theta} y^2 (\theta^2 - 2\theta y + y^2) \> \mathrm{d}y\\
        &= \frac{30}{\theta^5} \left[ \frac{1}{3}\theta^2 y^3 - \frac{1}{2}\theta y^4 + \frac{1}{5}y^5 \right]^{y=\mfrac{2}{3}\theta}_{y=0}\\
        &= 30 \left(\frac{1}{3}\right) \left(\frac{2}{3}\right)^3 - 15 \left(\frac{2}{3}\right)^4 + 6\left(\frac{2}{3}\right)^5\\
        &= \frac{64}{81}.
    \end{align*}

    \item The probability that $Y_{(5)}$ less than 10 minutes is equivalent to 
        taking the bus five times. That is 
        \begin{align*}
            \mathbb{P}[Y_{(5)} < 10] &= \mathbb{P}[Y_{(1)} < 10,\> Y_{(2)} < 10,\> \ldots, Y_{(5)} < 10]\\
            &= \mathbb{P}[Y_{(1)} < 10] \times \mathbb{P}[Y_{(2)} < 10] \times \cdots \mathbb{P}[Y_{(5)} < 10]\\
            &= \left( \frac{10}{15} \right)^5
            = \frac{32}{243}.
        \end{align*}
    \end{enumerate}
\end{solution}

\section*{Tutorials}

\begin{mdframed}
    \vspace{-0.25cm}
    \hspace{-0.25cm}
    \begin{Exercise}
        Let $X_1, X_2, \ldots, X_{2020}$ be a random sample of size $2020$ 
        from a Poisson distribution with density function
        \[
            f_{X_i}(x) = \frac{e^{-\lambda} \lambda^x}{x!}, \quad x = 0,1,2,\ldots,\infty.
        \]
        What is the distribution of $2020 \ols{X}$?
    \end{Exercise}
\end{mdframed}